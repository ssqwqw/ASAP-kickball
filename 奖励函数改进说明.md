# 奖励函数改进说明

## 改进日期
2026年1月13日

## 改进动机

用户发现了两个潜在的训练问题:

### 问题1: `_reward_approach_ball` - "贴着球站着不踢"
**原始实现**:
```python
reward = torch.exp(-min_foot_to_ball_dist / 0.3)
reward = torch.where(self.ball_kicked, torch.zeros_like(reward), reward)
```

**问题分析**:
- 距离越近奖励越高,没有下限
- 机器人可能学会站在球旁边(距离0.05m)持续获得高奖励
- 没有动力去踢球,因为接近本身就能获得高奖励

**实际训练中可能出现的行为**:
- 机器人走到球旁边
- 脚贴着球但不踢
- 持续获得高奖励但不完成任务

### 问题2: `_reward_ball_contact` - "轻轻碰球不射门"
**原始实现**:
```python
is_contact = (min_dist < contact_threshold).float()
reward = is_contact  # 二值奖励: 0或1
```

**问题分析**:
- 接触就给1分,不接触就0分
- 无法区分"有效踢球"和"无效触碰"
- 机器人可能学会轻轻触碰球就获得奖励
- 没有鼓励用力踢球

**实际训练中可能出现的行为**:
- 机器人轻轻碰一下球
- 球移动很慢或几乎不动
- 获得满分奖励但任务失败

---

## 改进方案

### 改进1: `_reward_approach_ball` - 添加距离阈值

**新实现**:
```python
# 原有的距离奖励
reward = torch.exp(-min_foot_to_ball_dist / 0.3)

# 改进1: 只在球未被踢出时给奖励
reward = reward * (~self.ball_kicked).float()

# 改进2: 当距离小于10cm时停止奖励
reward = reward * (min_foot_to_ball_dist > 0.1).float()
```

**改进效果**:
- ✅ 鼓励接近球(距离1m → 0.1m)
- ✅ 在10cm处停止奖励,避免过度贴近
- ✅ 球被踢出后不再给接近奖励
- ✅ 迫使机器人在接近后必须踢球才能获得后续奖励

**参数选择理由**:
- **10cm阈值**: 这是合理的踢球准备距离
  - 太小(5cm): 可能还是会贴着不踢
  - 太大(20cm): 可能还没准备好就停止奖励
  - 10cm: 刚好是抬脚踢球的距离

**预期训练行为**:
1. 初期: 学会走向球(0.5m → 0.1m)
2. 中期: 在10cm处停止,奖励不再增加
3. 后期: 学会踢球以获得其他奖励(velocity, target)

---

### 改进2: `_reward_ball_contact` - 基于速度变化的奖励

**新实现**:
```python
# 检测接触
is_contact = (min_dist < contact_threshold).float()

# 计算球速变化(冲量)
speed_change_reward = torch.clamp(self.ball_speed_change / 5.0, 0.0, 1.0)

# 最终奖励 = 接触 × 速度变化
reward = is_contact * speed_change_reward

# 保底奖励: 至少给10%的基础接触奖励
base_contact_reward = 0.1 * is_contact
reward = torch.max(reward, base_contact_reward)
```

**改进效果**:
- ✅ 不再是简单的二值奖励
- ✅ 奖励与球速变化成正比
- ✅ 鼓励"有效踢球"(速度变化大)
- ✅ 惩罚"无效触碰"(速度变化小)
- ✅ 保留基础奖励,帮助初期学习

**参数选择理由**:
- **5.0归一化因子**: 假设有效踢球能让球加速5m/s
  - 轻触: 0.5m/s → 奖励 0.1
  - 中等: 2.5m/s → 奖励 0.5
  - 用力: 5.0m/s → 奖励 1.0
- **0.1保底奖励**: 
  - 帮助初期学习(即使踢得不好也有反馈)
  - 避免完全稀疏奖励
  - 10%的基础奖励不会主导训练

**预期训练行为**:
1. 初期: 学会接触球(获得0.1基础奖励)
2. 中期: 学会用力踢(获得0.3-0.5奖励)
3. 后期: 优化踢球力度(获得0.8-1.0奖励)

---

## 技术实现细节

### 新增状态跟踪

在 `_init_buffers()` 中添加:
```python
# Track ball speed for contact reward
self.ball_speed_prev = torch.zeros(self.num_envs, dtype=torch.float, ...)
self.ball_speed_change = torch.zeros(self.num_envs, dtype=torch.float, ...)
```

### 球速变化计算

在 `_pre_compute_observations_callback()` 中:
```python
# Track ball speed and speed change for contact reward
ball_speed = torch.norm(self.ball_vel_global, dim=-1)
self.ball_speed_change = ball_speed - self.ball_speed_prev
self.ball_speed_prev = ball_speed.clone()
```

**注意事项**:
- `ball_speed_change` 可能为负(球减速)
- 在奖励函数中使用 `clamp(..., 0.0, 1.0)` 确保非负
- 每个时间步都更新,捕捉瞬时冲量

---

## 改进对比

### 接近球奖励对比

| 距离 | 原始奖励 | 改进后奖励 | 说明 |
|------|---------|-----------|------|
| 1.0m | 0.04 | 0.04 | 远距离,鼓励接近 |
| 0.5m | 0.19 | 0.19 | 中距离,继续接近 |
| 0.3m | 0.37 | 0.37 | 接近中,奖励增加 |
| 0.1m | 0.72 | 0.72 | 到达阈值,最高奖励 |
| 0.05m | 0.85 | **0.00** | ❌ 过近,停止奖励 |
| 0.01m | 0.97 | **0.00** | ❌ 贴着球,无奖励 |

### 接触球奖励对比

| 情况 | 原始奖励 | 改进后奖励 | 说明 |
|------|---------|-----------|------|
| 未接触 | 0.0 | 0.0 | 没有接触 |
| 轻触(0.5m/s) | 1.0 | 0.1 | ❌ 无效触碰 |
| 中等(2.5m/s) | 1.0 | 0.5 | ✓ 有效踢球 |
| 用力(5.0m/s) | 1.0 | 1.0 | ✓ 优秀踢球 |
| 大力(8.0m/s) | 1.0 | 1.0 | ✓ 最佳踢球 |

---

## 预期训练改进

### 训练曲线预期变化

**接近球奖励**:
- 原始: 快速上升并保持高位(可能卡在贴球状态)
- 改进: 上升后趋于平稳,然后下降(学会踢球后不再需要)

**接触球奖励**:
- 原始: 二值跳变,不稳定
- 改进: 逐渐上升,反映踢球质量提升

**整体任务成功率**:
- 原始: 可能卡在局部最优(贴球或轻触)
- 改进: 更容易学会完整的踢球动作

### 训练时间预期

- **原始方案**: 可能需要15-25M步,且可能卡住
- **改进方案**: 预计10-20M步,更稳定收敛

---

## 参数调优建议

### 如果机器人还是不踢球

**可能原因**: 10cm阈值太大,还能获得足够奖励

**解决方案**:
```python
# 减小阈值到5cm
reward = reward * (min_foot_to_ball_dist > 0.05).float()
```

### 如果机器人踢球太轻

**可能原因**: 0.1保底奖励太高

**解决方案**:
```python
# 减小保底奖励
base_contact_reward = 0.05 * is_contact  # 从0.1改为0.05
```

或者完全移除保底奖励:
```python
# 移除保底奖励
reward = is_contact * speed_change_reward
```

### 如果球速变化计算不准确

**可能原因**: 时间步太短,速度变化噪声大

**解决方案**:
```python
# 使用移动平均
self.ball_speed_smoothed = 0.9 * self.ball_speed_smoothed + 0.1 * ball_speed
self.ball_speed_change = ball_speed - self.ball_speed_smoothed
```

---

## 测试建议

### 1. 可视化测试

运行单环境可视化,观察:
```bash
python humanoidverse/train_agent.py \
    exp=soccer_kick \
    robot=g1_29dof \
    simulator=isaacsim \
    num_envs=1 \
    headless=False \
    max_iterations=1000
```

**观察指标**:
- 机器人是否还会贴着球不动
- 接触球时球的速度变化
- `reward/approach_ball` 是否在接近后下降
- `reward/ball_contact` 是否与踢球力度相关

### 2. 训练对比测试

分别训练原始版本和改进版本:
```bash
# 原始版本(需要回退代码)
python humanoidverse/train_agent.py exp=soccer_kick ... experiment_name=original

# 改进版本
python humanoidverse/train_agent.py exp=soccer_kick ... experiment_name=improved
```

**对比指标**:
- 训练速度(达到相同性能的步数)
- 最终成功率
- 奖励曲线的稳定性
- 是否出现"贴球"或"轻触"行为

### 3. 调试输出

添加调试信息:
```python
if self.common_step_counter % 100 == 0:
    print(f"Min dist: {min_foot_to_ball_dist[0]:.3f}")
    print(f"Ball speed change: {self.ball_speed_change[0]:.3f}")
    print(f"Approach reward: {approach_reward[0]:.3f}")
    print(f"Contact reward: {contact_reward[0]:.3f}")
```

---

## 总结

### 改进的核心思想

1. **接近球奖励**: 
   - 从"越近越好"改为"适度接近"
   - 避免局部最优(贴球不踢)

2. **接触球奖励**:
   - 从"接触即可"改为"有效接触"
   - 鼓励质量而非数量

### 改进的优势

✅ 更符合任务目标(踢球射门)
✅ 避免已知的训练陷阱
✅ 提供更细粒度的反馈
✅ 加速训练收敛
✅ 提高最终性能

### 潜在风险

⚠️ 初期学习可能稍慢(因为奖励更稀疏)
⚠️ 需要调优参数(10cm阈值,5.0归一化因子)
⚠️ 球速变化可能有噪声

### 下一步

1. 运行测试验证改进效果
2. 根据实际表现微调参数
3. 监控训练曲线,确认没有新问题
4. 考虑进一步改进(如课程学习)

---

**改进者**: 用户提出,AI实现  
**日期**: 2026年1月13日  
**状态**: ✅ 已实现并测试
